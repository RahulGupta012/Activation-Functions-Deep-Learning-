{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <div style=\"background-color: #3498db; padding: 10px; border-radius: 10px; color: #fff; text-align: center;\">Activation Function </div>\n",
        "\n"
      ],
      "metadata": {
        "id": "dzpbK0n7HL2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. What is an activation function in the context of artificial neural networks?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oFqKtoDwImlP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Artificial Neural network is a conception , which introduced by the insipiration of biological human mind and the it's neorounes.Which are the medium of the information.In the context of the deep learning, there is a artificial neroines by which we predict the sevral output by training of these neourones.As we learn things when we were a child.\n",
        "\n",
        "Activation function in the artificial neural networks is used to creat the value or criteira for classification of the datapoints.It is adding the non-linearity in the data and identifying the complex patterns in the data.Because of it's significance in deep learning ,it is called as 'Magic Souce' for neoural networks.There are sevral kind of Activation functions. Some of them is listed here;\n",
        "\n",
        "- Sigmoid Activation Function\n",
        "- Relu Activation Function\n",
        "- Leaky Relu Activation Function\n",
        "- Tanh (Hyperbolic Tangent) activation function\n",
        "- Softmax Activation Function\n",
        "- Parametric Relu Activation Function"
      ],
      "metadata": {
        "id": "-Xqgmax5Ju5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. What are some common types of activation functions used in neural networks?\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vSastbGfJu7H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are some common type of activation functions used in neural networks.We may discuss the breif introduction of all as in further quations we explained it as well.\n",
        "\n",
        "**Sigmoid Activation Function :**\n",
        "\n",
        "Sigmoid Activation function is very usefull in the case of binary classification problems.We used it at the output layer. It rages till 0 to 1.\n",
        "The contant value of this activation function is 0.5.This gives the probabilty as its output 0 to 1 . So we used it in the Fraud Detections, decese prediction..etc.\n",
        "\n",
        "**Relu :**\n",
        "\n",
        "Rectified Liner Unit is called as Relu. It is basically used in the hidden layers of the preceptrons as it tend to cease the value to become negative. In the other words it does'nt go the value beyond zero. So it's minimum range is zero while maximum range is infinity.\n",
        "\n",
        "- F(x)= max(0,x)\n",
        "\n",
        "where x should be infinite and the value of x should be determined by the function.\n",
        "\n",
        "\n",
        "**Softmax :**\n",
        "\n",
        "Softmax is also a very usefull activation function , which tells the different probabilities of the output. It is genarally used in the output layer for multi class classification problems."
      ],
      "metadata": {
        "id": "JLXTva6gOAIU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3. How do activation functions affect the training process and performance of a neural network?"
      ],
      "metadata": {
        "id": "YAzyt3sTKg7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Learning is known for it's capacity to solve the complex problems of the data. In such a case , Activation function plays a crucial role to solve such problems. It adds the capacity to classifying the non-liner datasets. Intially the perceptrons only have the capicity to classify the linerality seprable data. After introducing the concept of the Activation Function , the non-linearity also added in it. and it become more powerfull.These things leads to the model ,which is capable to identify the complex patterns so by the help of threshold we can predict the final outputs.\n",
        "\n",
        "It also increases the speed pf training as Activation functions like ReLU often lead to faster convergence compared to sigmoid or tanh.And  Activation functions impact the gradients during backpropagation, influencing how the weights are updated.\n"
      ],
      "metadata": {
        "id": "ehD3Ef1MKg9t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4. How does the sigmoid activation function work? What are its advantages and disadvantages?"
      ],
      "metadata": {
        "id": "zYOXrPOzKhAx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sigmoid Activation is the one of the popular activation function which is used for binary class classification. It gives the probabilty in their output. As in the probabilty, the range will 0 to 1. Similarly it's value will range till 0 to 1 as the output is nothing but probability. of the outputs.In other words this activation function is squashes the inputs to range 0 to 1.The farmula of sigmoid function is ;\n",
        "\n",
        " - sigma (x) = 1/1+e^x\n",
        "\n",
        "The intercept of this activation function is 0.5 . And it can be used to slove sevral real world problems such as ..farud Detection, Disease Predictions...etc.\n",
        "\n",
        "\n",
        "**Adavatanges of using Sigmoid Activation Function :**\n",
        "\n",
        "- It ranges to 0-1, which is the range of probability. so it gives the output in probabilty. and we can easily identify our output by the higher number of probability.\n",
        "\n",
        "- The sigmoid function is smooth and continuously differentiable. This smoothness property is beneficial during gradient-based optimization processes\n",
        "\n",
        "**Disadvatages of using the sigmoid activation function :**\n",
        "\n",
        "- Vanshing the gradient is an big disadvantage of the sigmoid activation function. As we discussed that the value of sigmoid activation function can never be negative.So when the input  moves away from 0 in either direction, the gradient approaches zero.\n",
        "\n",
        "- Sigmoid outputs tend to saturate (approach 0 or 1) for extreme values of input, making it harder for the model to learn during training. This is known as the saturation problem, and it can slow down the learning process.\n"
      ],
      "metadata": {
        "id": "EzpbktX9KhCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5.What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?"
      ],
      "metadata": {
        "id": "h1mAG2zYKhFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relu is also a popular activation function which is commenly used in the hidden layer. It has the capacity of solving the non-liner problems.It just ceases the inputs to be negative. It prevents the value to become negative and make them zero instead of. In other words ReLU introduces non-linearity by outputting the input directly if it is positive and zero otherwise.It ranges is 0 to infinity.\n",
        "The farmula of Relu ;\n",
        "\n",
        "- relu(x) = max (x,0) ,\n",
        "- where x tend to (- infinity , infinity)\n",
        "- Relu tend to (0, infinity)\n",
        "\n",
        "\n",
        "**Different from sigmoid activation function :**\n",
        "\n",
        "Relu is different from sigmoid activation function by certain aspects ;\n",
        "\n",
        "- Relu ranges (0,infinity) while sigmoid (0,1)\n",
        "\n",
        "- Relu is commenly used in hidden layer while sigmoid function is commenly used in output layer.\n",
        "- Relu is more capable for solving the non-linerity.\n",
        "- Relu is computetionally more efficient than sigmoid."
      ],
      "metadata": {
        "id": "bXBPFmVlKhHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q6. What are the benefits of using the ReLU activation function over the sigmoid function?"
      ],
      "metadata": {
        "id": "DFjmIa5IKhKr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are some benefits of using the Relu activation function over the sigmoid function. However it is completly dependent on the conditions and situation of our data and the type of the output that we need.Still some of the aspect are given , which are illusrating how the Relu is more benificial than sigmoid ;\n",
        "\n",
        "- Relu is more capable for solving the non-linerity.\n",
        "- Relu is computetionally more efficient than sigmoid.\n",
        "-  ReLU helps mitigate the issue of vanishing gradient which is a big problem, of sigmoid.As gradient of relu is either 0 (for negative inputs) or 1 (for positive inputs), providing stronger and more efficient signals during backpropagation."
      ],
      "metadata": {
        "id": "N7TxzUnQGiI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q7. Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem."
      ],
      "metadata": {
        "id": "gMejUt4xGiTN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leaky relu is a varient of Relu. Which have a additional feature of advance vanishing of gradient problem.The vanishing gradient problem occurs when the gradient of the activation function becomes very small for certain input values, leading to slow or stalled learning during backpropagation. Leaky ReLU addresses this issue by allowing a small gradient for negative inputs, preventing them from being completely disregarded during weight updates.Farmula of leaky relu\n",
        "\n",
        "- x if x 0\n",
        "- lambda.x if x <_ 0\n",
        "\n",
        "x is a very small postive value."
      ],
      "metadata": {
        "id": "GuZ0NdY4KhMO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q8. What is the purpose of the softmax activation function? When is it commonly used?"
      ],
      "metadata": {
        "id": "w73MfMcAKhPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The softmax activation function is a very useful activation function which have capability of solving the multi class classification problems. We used this function at the output layer as it also gives the probabiliy of outputs as the sigmoid activation function.Softmax activation is often paired with the cross-entropy loss function during training. The cross-entropy loss measures the dissimilarity between the predicted probability distribution and the true distribution. The common usecase of this activation function in a real world are Speech reconization, natural language processing (NLP) and image classification."
      ],
      "metadata": {
        "id": "io0K5NgVIU-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?"
      ],
      "metadata": {
        "id": "OYof0k8AIVWt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tanh and sigmoid activation function are similer in some aspects , while disimiller in other aspects. As they are not same in the range as where sigmoid range is 0 to 1, tanh rages till -1 to 1. In the case of the output the sigmoid function gives the output in probabilty as it ranges 0 to 1 while the tanh have not the output of probablity.The farmula of tanh given below\n",
        "\n",
        "- tanh = (e^x - e^-x) / (e^x + e^-x)\n",
        "\n",
        "Both sigmoid and tanh can suffer from the vanishing gradient problem, where the gradients become very small during backpropagation.The tanh function involves exponentials and can be computationally more expensive than the sigmoid function."
      ],
      "metadata": {
        "id": "SAI9L1acIVoy"
      }
    }
  ]
}